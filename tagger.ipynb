{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# myChess Tagger\n",
    "\n",
    "This notebook contains the implementation for our tagger.\n",
    "\n",
    "Because this is mostly a proof of concept, the notebook paradigm seems best suited to explaining methodology as it occurs.\n",
    "It would not be difficult to recompile this to a standalone .py file for deployment in a production environment.\n",
    "\n",
    "\n",
    "We can begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bill\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\Bill\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then import our data sets and begin necessary preprocessing like cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggro = pd.read_csv(\"aggressive.csv\")\n",
    "defen = pd.read_csv(\"defensive.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our defensive data set is slightly larger than our aggressive data set, but it is not too large of a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11118\n",
      "11573\n"
     ]
    }
   ],
   "source": [
    "print(len(aggro))\n",
    "print(len(defen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Event Site        Date Round                White  \\\n",
      "0  Earl tourn    ?  1906.??.??     ?               Savrov   \n",
      "1  Earl tourn    ?  1906.??.??     ?                Giese   \n",
      "2  Earl tourn    ?  1906.??.??     ?  Alekhine, Alexander   \n",
      "3  Earl tourn    ?  1906.??.??     ?  Alekhine, Alexander   \n",
      "4  Earl tourn    ?  1906.??.??     ?            Manko, V.   \n",
      "\n",
      "                 Black  Result  BlackElo  WhiteElo  ECO  Move Count  \\\n",
      "0  Alekhine, Alexander     0-1       NaN       NaN  C30          44   \n",
      "1  Alekhine, Alexander     0-1       NaN       NaN  C47          17   \n",
      "2         Ljubimov, T.  Jan-00       NaN       NaN  C58          38   \n",
      "3           Romaskevic  Jan-00       NaN       NaN  C20          18   \n",
      "4  Alekhine, Alexander  Jan-00       NaN       NaN  C52          35   \n",
      "\n",
      "   Average Material Threatened  Gambit Count  Check Count  \\\n",
      "0                     1.840909             0            8   \n",
      "1                     1.235294             0            1   \n",
      "2                     1.815789             1            4   \n",
      "3                     2.222222             0            1   \n",
      "4                     1.057143             1            0   \n",
      "\n",
      "   Average Board Evaluation  Aggressive  \n",
      "0                  7.381818           1  \n",
      "1                 11.405882           1  \n",
      "2                  8.894737           1  \n",
      "3                  6.811111           1  \n",
      "4                  2.788571           1  \n"
     ]
    }
   ],
   "source": [
    "print(aggro.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then drop data elements that won't be useful for training purposes and encode the ECO field.\n",
    "\n",
    "ECO represents the opener that occurred for the game in question, and so we choose to encode this to get an idea of what openers occur in aggressive and defensive players' games respectively.\n",
    "\n",
    "After dropping and enconding we can begin train/test splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Aggressive', 'Average Board Evaluation', 'Average Material Threatened',\n",
      "       'Check Count', 'ECO', 'Gambit Count', 'Move Count'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "total = pd.concat([aggro, defen])\n",
    "drops = [\"Black\", \"BlackElo\", \"BlackTeam\", \"BlackTeamCountry\", \"Date\", \"Event\", \n",
    "         \"EventCategory\", \"EventCountry\", \"EventDate\", \"EventRounds\", \"EventType\", \"FEN\", \"PlyCount\", \"Result\",\n",
    "        \"Round\", \"SetUp\", \"Site\", \"Source\", \"SourceDate\", \"White\", \"WhiteElo\", \"WhiteTeam\", \"WhiteTeamCountry\"]\n",
    "total = total.drop(drops, axis=1)\n",
    "\n",
    "number = LabelEncoder()\n",
    "total[\"ECO\"] = number.fit_transform(total[\"ECO\"].astype('str'))\n",
    "print(total.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = total.drop([\"Aggressive\"], axis=1)\n",
    "y = total[\"Aggressive\"]\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below are mostly for testing purposes to achieve some satisfactory number of decision trees in our forest. Usually the more the better but for the sake of this application we can stop at some low number (i.e. 200 or less), especially since the accuracy doesn't fluctuate too much after a certain point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "classifiers = []\n",
    "nmin = 1\n",
    "nmax = 200\n",
    "for n in range(nmin, nmax):\n",
    "    forest = RandomForestClassifier(n_estimators = n)\n",
    "    forest.fit(X_train, y_train)\n",
    "    classifiers.append(forest)\n",
    "    scores.append(forest.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amax = np.argmax(scores)\n",
    "print(scores[amax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([i for i in range(amax - 50, amax + 2)])\n",
    "scores = np.array(scores)\n",
    "\n",
    "plt.figure(figsize=(20,5));\n",
    "plt.plot(x, [r * 100 for r in [scores[i] for i in x]]);\n",
    "plt.xticks(x, [str(i) for i in x])\n",
    "plt.grid(True)\n",
    "plt.ylabel(\"% Accuracy\")\n",
    "plt.xlabel(\"n Estimators\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best number of trees from our trials: \", amax)\n",
    "print(\"The accuracy achieved with that number of trees:\" , scores[amax])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method, provided <a href=\"http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\">here,</a> gives us a convenient means of plotting our confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = classifiers[amax]\n",
    "y_pred = f.predict(X_test)\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=[\"Aggressive\", \"Defensive\"],\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=[\"Aggressive\", \"Defensive\"], normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then begin to examine cross validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvscore = cross_val_score(f, X_train, y_train, cv=10)\n",
    "print(np.mean(cvscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can briefly examine some configurations via Grid Search to see what performance and accuracy may be possible under certain conditions for our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_jobs=-1)\n",
    "arr = [i*5 for i in range(1, 20)]\n",
    "param_grid = {\n",
    "    'n_estimators': arr, # up to 100 trees\n",
    "    'max_depth': arr,\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "grid_clf = GridSearchCV(clf, param_grid, cv=10)\n",
    "grid_clf.fit(X_train, y_train)\n",
    "print(grid_clf.best_params_)\n",
    "print(grid_clf.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_clf.best_params_)\n",
    "print(grid_clf.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = grid_clf.best_estimator_\n",
    "print(f.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
